[project]
name = "bazel-pyproject"
version = "0"
requires-python = ">=3.9" # NOTE: Must be kept in sync with other global settings
dependencies = [
  "absl-py",
  "aiofiles",
  "aiohttp",
  "asgiref",
  "async-asgi-testclient",
  "boto3",
  "click>=8.1",
  "datasets",
  "device-smi",
  "docutils",
  "einops",
  "einx",
  "fastapi>=0.115.3", # >=0.115.3 is required for resolving CVE-2024-47874
  "faster_whisper",
  "fire ", # For mistral-evals.
  "filelock",
  "gguf",
  "hf-transfer",
  "httpx==0.27.2",
  "huggingface-hub>=0.27.1",
  "hypothesis==6.108.4",
  "ipython",
  "jinja2",
  "jiwer",
  "kaleido==0.2.1",
  "llguidance",
  "logbar",
  "markupsafe",
  "matplotlib",
  "msgspec",
  "munch",
  "mypy",
  "notebook",
  "numpy",
  "nvitop",
  "onnxruntime==1.19.2 ", # Needed by faster_whisper, newer versions don't support python 3.9
  "opentelemetry-api",
  "opentelemetry-exporter-otlp-proto-http>=1.28.2",
  "opentelemetry-exporter-prometheus",
  "opentelemetry-sdk",
  "packaging",
  "pre-commit",
  "prometheus-client",
  "protobuf==6.31.1 ", # Must match bazel protobuf version since it doesn't the codegen
  "psutil>=5.9.0", # needed for setting `--timeout` for llvm-lit
  "py-cpuinfo",
  "pyarrow",
  "pydantic",
  "pydantic-settings==2.3.4",
  "pygame",
  "pygments",
  "pyinstrument",
  "pytest-asyncio==0.23.7",
  "pytest-benchmark",
  "pytest-mock",
  "pytest-xdist",
  "pytest>=7.2",
  "python-json-logger",
  "PyYAML",
  "pyzmq",
  "plotly",
  "qwen-vl-utils",
  "requests>=2.28",
  "regex",
  "safetensors",
  "sentencepiece",
  "setuptools",
  "simpy",
  "sphinx==7.4.7",
  "sse-starlette==2.1.2",
  "sseclient-py",
  "starlette==0.47.2", # transitively included by sse-starlette, >=0.40.0 is required for resolving CVE-2024-47874, 0.41.3 introduces a breaking change against mypy (https://github.com/encode/starlette/discussions/2757)
  "stack-pr",
  "taskgroup",
  "termcolor",
  "threadpoolctl",
  "tokenicer",
  "tomli",
  "tqdm",
  "transformers>=4.51.1 ", # v4.47.0 and v4.49.0 introduced changes that break our torch.compile backend.
  "tokenizers",
  "typing-extensions",
  "uvloop>=0.21.0",
  "uvicorn",
  "wheel",

  # modeltool
  "google-auth==2.29.0",
  "google-cloud-bigquery==3.22.0",
  "pandas>=1.4.2",
  "pyasn1",
  "requests>=2.28.0",
  "rich>=12.4.4",
  "scipy>=1.10.1",
  "tabulate>=0.8.9",

  # benchmarks
  "schema==0.7.5",

  # perfsect_v2
  "responses>=0.23.3",

  # mblack
  "mypy-extensions>=0.4.3",
  "pathspec>=0.9.0",
  "platformdirs>=2",

  # pillow is required to get the torch tests configured correctly
  # in the wheel build.
  "pillow>=10.0.0",
  "locust",
  "openai==1.52.2",
  "librosa==0.10.2",
  "soundfile==0.12.1",
  "grpcio",
  "pytest-grpc",

  # mypy types
  "types-protobuf",
  "types-PyYAML",
  "types-setuptools",
  "types-tabulate",

  # tts
  "zhon",
  "zhconv",
]

[dependency-groups]
amd = [
  "torch==2.7.0",
  "torchaudio==2.7.0",
  "torchvision==0.22.0",

  "gptqmodel; sys_platform == 'linux' and platform_machine == 'x86_64'",
  "pytorch-triton-rocm; sys_platform == 'linux' and platform_machine == 'x86_64'",

  "accelerate",
  "lm-eval[api,ifeval,math]",
  "mteb",
  "optimum",
  "sentence-transformers",
  "timm",
  "torchmetrics",
]

cpu = [
  "torch==2.7.0",
  "torchaudio==2.7.0",
  "torchvision==0.22.0",

  "accelerate",
  "lm-eval[api,ifeval,math]",
  "mteb",
  "optimum",
  "sentence-transformers",
  "timm",
  "torchmetrics",
]

nvidia = [
  "torch==2.7.0",
  "torchaudio==2.7.0",
  "torchvision==0.22.0",

  "gptqmodel; sys_platform == 'linux' and platform_machine == 'x86_64'",

  "accelerate",
  "lm-eval[api,ifeval,math]",
  "mteb",
  "optimum",
  "sentence-transformers",
  "timm",
  "torchmetrics",
]

[tool.uv]
default-groups = ["cpu"]
conflicts = [
  [
    { group = "amd" },
    { group = "cpu" },
    { group = "nvidia" },
  ],
]

environments = [
    "sys_platform == 'darwin'",
    "sys_platform == 'linux'",
]

[tool.uv.sources]
torch = [
  { index = "pytorch-amd" , group = "amd" },
  { index = "pytorch-cpu", group = "cpu" },
  { url = "https://modular-bazel-artifacts-public.s3.amazonaws.com/artifacts/torch/2.7.0/76be807ccbeeca14632a7a49c3737b9b4e4b8f57e78889730314de0807d28bb7/torch-2.7.0%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl", group = "nvidia" },
]
torchvision = [
  { index = "pytorch-amd" , group = "amd" },
  { index = "pytorch-cpu", group = "cpu" },
  { index = "pytorch-nvidia", group = "nvidia" },
]
torchaudio = [
  { index = "pytorch-amd" , group = "amd" },
  { index = "pytorch-cpu", group = "cpu" },
  { index = "pytorch-nvidia", group = "nvidia" },
]
pytorch-triton-rocm = [
  { index = "pytorch-amd" },
]
gptqmodel = [
  { url = "https://github.com/ModelCloud/GPTQModel/releases/download/v2.0.0/gptqmodel-2.0.0+cu126torch2.6-cp312-cp312-linux_x86_64.whl" },
]

[[tool.uv.index]]
name = "pytorch-amd"
url = "https://download.pytorch.org/whl/rocm6.3"
explicit = true

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-nvidia"
url = "https://download.pytorch.org/whl/cu128"
explicit = true
